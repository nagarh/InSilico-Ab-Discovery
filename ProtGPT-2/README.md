# Generating Antibody Sequences in silico with ProtGPT-2
The existing ProtGPT-2 model has been specifically trained on the most frequently occurring V gene combinations, particularly the IGHV3-IGKV1 pair, derived from human paired antibody sequence (Fv regions only) datasets. This training enables the model to generate diverse and realistic antibody sequences in silico, closely mimicking natural antibody repertoires.The generated sequences are then modeled into three-dimensional structures using IgFold, a deep learning-based framework designed for rapid and accurate antibody structure prediction. PyRosetta is used to further enhance the quality and precision of these structures, ensuring that the modeled antibodies are both physically realistic and suitable for downstream structural and functional analyses. 


## Directory Structure

```
ProtGPT-2/
├── data/
│   ├── processed/ # Model input sequences (<vh|vl>) for training
│   │   ├── Prot_input_seq_train_data.csv # training data
│   │   ├── Prot_input_seq_val_data.csv   # validation data
│   │   └── Prot_input_seq.csv  # train + val data
│   ├── raw/
│   │   ├── IGHV3_IGKV1_Paired_BType_Naive_B_Cells_Disease_None.csv
│   ├── generated_sequences.txt
│   ├── filtered_sequences.fasta
│   ├── Training_loss.csv
│   ├── Training_loss.png
│   ├── Validation_loss.png
│   ├── predicted_structures/ # structures of antibodies generated by the model 

├── logs/
│   └── events.out.tfevents.* # model training logs
├── models/
│   ├── checkpoints/
│   └── protgpt2_antibody_model/
│       ├── added_tokens.json
│       ├── config.json
│       ├── generation_config.json
│       ├── merges.txt
│       ├── model.safetensors
│       ├── tokenizer_config.json
│       ├── tokenizer.json
│       ├── vovab.json
│       └── special_tokens_map.json
├── notebooks/
│   ├── Data_processing.ipynb # notebook to write input sequence format for model
│   └── train.ipynb # training noteboook of model
├── scripts/
│   ├── Sample_sequences.py # code to generate new sequences from trained model
│   ├── filter_sequences.py # code to check validity of generated sequences
│   ├── predict_seq_structure.py # code to generate structure of generated sequences
│   └── train.py # model training script (PyTorch library is used to train the model)
└── README.md

```

## Folder Descriptions

### [`data/`](./data)
Contains the datasets used for training and validation

- [`processed/`](./data/processed): Preprocessed data ready for model training and validation
  - `Prot_input_seq_train_data.csv`: Training sequences
  - `Prot_input_seq_val_data.csv`: Validation sequences
  - `Prot_input_seq.csv`: Combined sequences
  
- [`raw/`](./data/raw): Raw data files before preprocessing.
  - `IGHV3_IGKV1_Paired_BType_Naive_B_Cells_Disease_None.csv`: Raw antibody sequences
- `Training_loss.csv`: Training loss data
- `Training_loss.png`
- `Validation_loss.png`

- [`predicted_structures/`](./data/predicted_structures) : Structure of the antibody sequences generated by the deep learning model

### [`logs/`](./logs)
Contains TensorBoard log files for visualizing training metrics

- `events.out.tfevents.*`: TensorBoard event files

### [`models/`](./models)
Contains the trained models and checkpoints

- [`checkpoints/`](./models/checkpoints): Directory for saving model checkpoints during training
- [`protgpt2_antibody_model/`](./models/protgpt2_antibody_model/): Directory for the final trained model and tokenizer.
  - `added_tokens.json`: Additional tokens added to the tokenizer
  - `config.json`: Model configuration file
  - `generation_config.json`: Configuration for text generation
  - `merges.txt`: Byte pair encoding merges
  - `model.safetensors`: Trained model weights
  - `special_tokens_map.json`: Mapping of special tokens
  - `tokenizer_config.json` : Configuration of tokenizer
  - `tokenizer.json` : Model tokenizer
  - `vocab.json` : Model vocabulary

### [`notebooks/`](./notebooks/)
Jupyter notebooks for data processing and training.

- [`Data_processing.ipynb`](./notebooks/Data_processing.ipynb): Notebook to process model input antibody sequences and write in input format (<vh|vl>)
- [`train.ipynb`](./notebooks/train.ipynb): Notebook for training and evaluating the model

### [`scripts/`](./scripts/)

- [`train.py`](./scripts/train.py): Script for training the ProtGPT-2 model
- [`Sample_sequences.py`](./scripts/Sample_sequences.py) : Script for generating new sequences from the trained model
- [`filter_sequences.py`](./scripts/filter_sequences.py) : Script for filtering out sequences that are too short or too long or not within the range of heavy and light chain sequence length
- [`predict_seq_structure.py`](./scripts/predict_seq_structure.py) : Script for generating antibody structures from the generated sequences. Igfold and PyRosetta is used to generate antibody structures

## Usage

To load the trained ProtGPT-2 model using the `transformers` library, follow these steps:
- model weights can be downloaded from the following link (model.safetensors):
https://drive.google.com/file/d/1vpVOwoRZ7Fhad6fXjJR9EPSDbcxF3J3D/view?usp=drive_link


1. Install the `transformers` library :
    ```sh
    pip install transformers
    ```

2. Download the trained model and load both the model and tokenizer :
    ```python
    from transformers import AutoModelForCausalLM, AutoTokenizer

    model_path = "./models/protgpt2_antibody_model"

    # Load the tokenizer
    tokenizer = AutoTokenizer.from_pretrained(model_path)

    # Load the model
    model = AutoModelForCausalLM.from_pretrained(model_path)


### Model training parameters 
* Optimizer - Adam ( learning rate: 5e-4, weight_decay:1e-5)
* batch size -  16
* train/validation - 90/10
* The model was trained on a single NVIDIA H100 GPU with a batch size of 16 and for 2 epochs

### References
- Existing model is taken from Hugging face : https://huggingface.co/nferruz/ProtGPT2
- Research paper : https://doi.org/10.1038/s41467-022-32007-7





